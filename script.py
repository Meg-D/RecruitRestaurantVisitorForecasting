# -*- coding: utf-8 -*-
"""final-notebook.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13ZRk8QRAalfIdAxHIWXUmNGqZP3lpqAT
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as pyplot

from google.colab import drive
drive.mount("/content/drive")

"""# **Reading csv files**"""

hpgst=pd.read_csv('/content/drive/My Drive/ml-project/restaurant-visitor-forecasting/hpg_store_info.csv')
airst=pd.read_csv('/content/drive/My Drive/ml-project/restaurant-visitor-forecasting/air_store_info.csv')
sub=pd.read_csv('/content/drive/My Drive/ml-project/restaurant-visitor-forecasting/sample_submission.csv')
test=pd.read_csv('/content/drive/My Drive/ml-project/restaurant-visitor-forecasting/test.csv')
train=pd.read_csv('/content/drive/My Drive/ml-project/restaurant-visitor-forecasting/train.csv')
hpgre=pd.read_csv('/content/drive/My Drive/ml-project/restaurant-visitor-forecasting/hpg_reserve.csv')
airre=pd.read_csv('/content/drive/My Drive/ml-project/restaurant-visitor-forecasting/air_reserve.csv')
date=pd.read_csv('/content/drive/My Drive/ml-project/restaurant-visitor-forecasting/date_info.csv')
rel=pd.read_csv('/content/drive/My Drive/ml-project/restaurant-visitor-forecasting/store_id_relation.csv')

"""**EDA**- EXPLORATORY DATA ANALYSIS"""

#number of visitors per day
air_visit_visitors_PerDay = pd.DataFrame(train.groupby("visit_date")["visitors"].sum()).reset_index()
air_visit_visitors_PerDay.index +=1

air_visit_visitors_PerDay.head()

air_visit_visitors_PerDay["visit_date"]=pd.to_datetime(air_visit_visitors_PerDay["visit_date"])

pyplot.plot(air_visit_visitors_PerDay["visit_date"],air_visit_visitors_PerDay["visitors"])

pyplot.xlabel("visit-date")
pyplot.ylabel("visitors per day")
pyplot.title("All Visitors Per day From 23-April 2016 to 22-April 2017")
pyplot.figure(figsize=(100,100))
pyplot.figure().subplots_adjust(bottom=0.2)
pyplot.show()

#total visitors per weekday

air_visit_visitors_PerDay["weekday"]=air_visit_visitors_PerDay["visit_date"].apply(lambda x: x.weekday())

b =pd.DataFrame({"week" : ["Monday","Tuesday","Wednesday","Thursday","Friday","Saturday","Sunday"],
"weekday" : [0,1,2,3,4,5,6]})
b.index +=1

air_visit_visitors_PerDay= air_visit_visitors_PerDay.merge(b,on="weekday")

x=pd.DataFrame(air_visit_visitors_PerDay.groupby("week")["visitors"].median()).reset_index()

pyplot.bar(x["week"],x["visitors"])
pyplot.xlabel("Weekday")
pyplot.ylabel("visitors per weekday")
pyplot.title("All visitors per weekday")
pyplot.show()

#visitors grouped by month

air_visit_visitors_PerDay["month"]=air_visit_visitors_PerDay["visit_date"].dt.month
c =pd.DataFrame({"Month" : ["Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec"],
"month" : [1,2,3,4,5,6,7,8,9,10,11,12]})
c.index+=1
air_visit_visitors_PerDay = air_visit_visitors_PerDay.merge(c)

del air_visit_visitors_PerDay["month"]
del air_visit_visitors_PerDay["weekday"]

z=pd.DataFrame(air_visit_visitors_PerDay.groupby("Month")["visitors"].median()).reset_index()

pyplot.bar(z["Month"],z["visitors"])
pyplot.xlabel("month")
pyplot.ylabel("visitors per month")
pyplot.title("All Visitors per Month")
pyplot.show()

#getting the most popular genre
gen=airst['air_genre_name'].value_counts()
gen.sort_values().plot(kind='barh',figsize=(10,5))

train.rename(columns={'visit_date':'date'},inplace='True')
date['date']=date['calendar_date']
date.drop({'calendar_date'},axis=1,inplace=True)

datemerge = pd.merge(date,train,on='date')

datemerge['id'] = datemerge['air_store_id'].map(str)+ '_' + datemerge['date'].map(str)
datemerge['month']=pd.DatetimeIndex(datemerge['date']).month
datemerge['weekday']=pd.DatetimeIndex(datemerge['date']).weekday
datemerge['year']=pd.DatetimeIndex(datemerge['date']).year

#calculating the count of visitors each weekday.

d=datemerge.groupby('weekday')['visitors'].sum()
d.plot(kind='bar',figsize=(10,5),ylabel="number of visitors")

#holiday flag
te=datemerge.groupby('weekday')['holiday_flg'].sum()
sns.countplot(x='weekday',hue='holiday_flg',data=datemerge)

te=datemerge.groupby('holiday_flg')['visitors'].sum()
te.plot(kind='bar',figsize=(10,5),ylabel='number of visitors')

air = pd.merge(airre,airst,on='air_store_id')


air["visit_datetime"]= pd.to_datetime(air["visit_datetime"])
air["visit_date"] = air["visit_datetime"].dt.date
air["visit_date"] = pd.to_datetime(air["visit_date"])
air['DayofWeek'] = air['visit_date'].dt.dayofweek
air['year'] = air['visit_date'].dt.year
air['month'] = air['visit_date'].dt.month
air['visit_date'] = air['visit_date'].dt.date
air['id'] = air['air_store_id'].map(str) + '_' + air['visit_date'].map(str)

air

tem=pd.merge(datemerge,air,on='id')
#tem.drop({'air_store_id_y','visit_datetime','day_of_week','year','month','air_area_name'},axis=1,inplace=True)
tem.rename(columns={'air_store_id_x':'air_store_id'},inplace='True')
tem

#getting the area with maximum number of visitors
tem['prefecture']=tem['air_area_name'].str.split().str[0]
tem['city']=tem['air_area_name'].str.split().str[1]
tem['area']=tem['air_area_name'].str.split().str[2]

A=tem.groupby('prefecture')['visitors'].sum()
A.sort_values().plot(kind='barh',figsize=(10,5),ylabel="number of visitors")

#number of visitors by genre name
A=tem.groupby('air_genre_name')['visitors'].sum()
A.plot(kind='bar',figsize=(10,5),ylabel="number of visitors")

my = sns.countplot(x='month_y',hue="year_y", data=tem)

"""**Merging datasets**"""

#dividing area name into three components - 1. Prefecture 2. City 3. Area

airst['prefecture']=airst['air_area_name'].str.split().str[0]
airst['city']=airst['air_area_name'].str.split().str[1]
airst['area']=airst['air_area_name'].str.split().str[2]
airst

#date['date']=date['calendar_date']
#date.drop({'calendar_date'},axis=1,inplace=True)

#train.rename(columns={'visit_date':'date'},inplace='True')

#merging date with training data to get number of visitors each day.

#datemerge = pd.merge(date,train,on='date')

#datemerge['id'] = datemerge['air_store_id'].map(str)+ '_' + datemerge['date'].map(str)
#datemerge['month']=pd.DatetimeIndex(datemerge['date']).month
#datemerge['weekday']=pd.DatetimeIndex(datemerge['date']).weekday
#datemerge['year']=pd.DatetimeIndex(datemerge['date']).year

datemerge.drop({'day_of_week'},axis=1,inplace=True)

datemerge

train["date"] = pd.to_datetime(train["date"])
train['DayofWeek'] = train['date'].dt.dayofweek
train['year'] = train['date'].dt.year
train['month'] = train['date'].dt.month
train['date'] = train['date'].dt.date
train

#merging air store info with date info

temp=datemerge.merge(airst,on=['air_store_id'],how='left')
temp

#formatting the submission dataset

sub["visit_date"]=sub["id"].map(lambda x: str(x).split("_")[2])
sub["air_store_id"]=sub["id"].map(lambda x: "_".join(x.split("_")[:2]))

#creating a new dataset by taking air store ids and day of week as this is an important feature

distinct_stores=sub["air_store_id"].unique()
stores=pd.concat([pd.DataFrame({"air_store_id":distinct_stores,"DayofWeek":[i]*len(distinct_stores)}) for i in range(7)],axis=0,ignore_index=True).reset_index(drop=True)

#taking mean, median, count, minimum, maximum of number of visitors according to day of week

t = train.groupby(["air_store_id","DayofWeek"],as_index=False)["visitors"].min().rename(columns={"visitors":"min_visitors"})
stores=pd.merge(stores,t,how="left",on=["air_store_id","DayofWeek"])

t = train.groupby(["air_store_id","DayofWeek"],as_index=False)["visitors"].mean().rename(columns={"visitors":"mean_visitors"})
stores= pd.merge(stores,t,how="left",on=["air_store_id","DayofWeek"])

t = train.groupby(["air_store_id","DayofWeek"],as_index=False)["visitors"].median().rename(columns={"visitors":"median_visitors"})
stores= pd.merge(stores,t,how="left",on=["air_store_id","DayofWeek"])

t = train.groupby(["air_store_id","DayofWeek"],as_index=False)["visitors"].max().rename(columns={"visitors":"max_visitors"})
stores= pd.merge(stores,t,how="left",on=["air_store_id","DayofWeek"])

t = train.groupby(["air_store_id","DayofWeek"],as_index=False)["visitors"].count().rename(columns={"visitors":"count_visitors"})
stores= pd.merge(stores,t,how="left",on=["air_store_id","DayofWeek"])

stores = stores.rename(columns={'DayofWeek':'weekday'})
stores

#merging this new dataset with the initially merged dataset.

full=pd.merge(temp,stores,how="left",on=["air_store_id","weekday"])
full

"""### Adding some new features"""

#standard deviation of number of visitors grouped by prefecture.
pre_std = full.groupby('prefecture').agg({'visitors': 'std'})
pre_std = pre_std.rename(columns={'visitors':'pre_std_visitor'})
pre_std = pre_std.reset_index()

#mean of number of visitors grouped by prefecture.
pre_mean = full.groupby('prefecture').agg({'visitors': 'mean'})
pre_mean = pre_mean.reset_index()
pre_mean = pre_mean.rename(columns={'visitors':'pre_avg_visitor'})

#mean of number of visitors grouped by genre.
gen_mean = full.groupby('air_genre_name').agg({'visitors': 'mean'})
gen_mean = gen_mean.rename(columns={'visitors':'gen_avg_visitor'})
gen_mean = gen_mean.reset_index()

#standard deviation of number of visitors grouped by genre.
gen_std = full.groupby('air_genre_name').agg({'visitors': 'std'})
gen_std = gen_std.rename(columns={'visitors':'gen_std_visitor'})
gen_std = gen_std.reset_index()

#standard deviation of number of visitors grouped by city.
city_std = full.groupby('city').agg({'visitors': 'std'})
city_std = city_std.rename(columns={'visitors':'city_std_visitor'})
city_std = city_std.reset_index()

#mean of number of visitors grouped by city.
city_mean = full.groupby('city').agg({'visitors': 'mean'})
city_mean = city_mean.rename(columns={'visitors':'city_avg_visitor'})
city_mean = city_mean.reset_index()

#standard deviation of number of visitors grouped by area
area_std = full.groupby('area').agg({'visitors': 'std'})
area_std = area_std.rename(columns={'visitors':'area_std_visitor'})
area_std = area_std.reset_index()

#mean of number of visitors grouped by area.
area_mean = full.groupby('area').agg({'visitors': 'mean'})
area_mean = area_mean.rename(columns={'visitors':'area_avg_visitor'})
area_mean = area_mean.reset_index()

#merging these new features to our original set.
full=pd.merge(full,pre_std,how="left",on=["prefecture"])
full=pd.merge(full,pre_mean,how="left",on=["prefecture"])
full=pd.merge(full,gen_mean,how="left",on=["air_genre_name"])
full=pd.merge(full,gen_std,how="left",on=["air_genre_name"])
full=pd.merge(full,city_std,how="left",on=["city"])
full=pd.merge(full,city_mean,how="left",on=["city"])
full=pd.merge(full,area_mean,how='left',on=['area'])
full=pd.merge(full,area_std,how='left',on=['area'])
full

"""## MODEL 1- LINEAR REGRESSION

**Encoding the Categorical Features**
"""

#one hot encoding

g = pd.get_dummies(full.air_genre_name, prefix='G')
c = pd.get_dummies(full.city, prefix='C')
p = pd.get_dummies(full.prefecture, prefix='P')
a = pd.get_dummies(full.area,prefix='A')

final1 = pd.concat([full,g,c,p,a], axis=1, sort=False)

del final1['air_genre_name']
del final1['prefecture']
del final1['city']
del final1['area']

#final dataset for training the model

final1

"""Formating the test dataset similarly"""

test1=sub

test1["visit_date"] = pd.to_datetime(test["visit_date"])
test1["visit_weekday"] = test1["visit_date"].dt.dayofweek

date["visit_date"] = pd.to_datetime(date["date"])
date

test1 = pd.merge(test1,date,how="left",on=["visit_date"])
test1

del test1['day_of_week']

test1 = pd.merge(test1,airst,how="left",on=["air_store_id"])
test1

test1.drop({'air_area_name','visit_date'},axis=1,inplace=True)

test1=pd.merge(test1,pre_std,how="left",on=["prefecture"])
test1=pd.merge(test1,pre_mean,how="left",on=["prefecture"])
test1=pd.merge(test1,gen_mean,how="left",on=["air_genre_name"])
test1=pd.merge(test1,gen_std,how="left",on=["air_genre_name"])
test1=pd.merge(test1,city_std,how="left",on=["city"])
test1=pd.merge(test1,city_mean,how="left",on=["city"])
test1=pd.merge(test1,area_mean,how='left',on=['area'])
test1=pd.merge(test1,area_std,how='left',on=['area'])
test1

test1["date"] = pd.to_datetime(test1["date"])
test1['year'] = test1['date'].dt.year
test1['month'] = test1['date'].dt.month
test1['date'] = test1['date'].dt.date

test1

#encoding the test dataset

g1 = pd.get_dummies(test1.air_genre_name, prefix='G')
c1 = pd.get_dummies(test1.city, prefix='C')
p1 = pd.get_dummies(test1.prefecture, prefix='P')
a1 = pd.get_dummies(test1.area,prefix='A')

test1 = pd.concat([test1,g1,c1,p1,a1], axis=1, sort=False)

del test1['prefecture']
del test1['air_genre_name']
del test1['city']
del test1['area']

test1 = test1.rename(columns={'visit_weekday':'weekday'})
test1

test1 = pd.merge(test1,stores,how="left",on=["air_store_id","weekday"])
test1

"""### MODEL TRAINING"""

from sklearn.linear_model import LinearRegression
lm = LinearRegression()

final1.drop({'air_area_name'},axis=1,inplace=True)

y1=final1['visitors']

#removing all the columns that are not integer or float
#id, store_id is string
#date is date format
#visitors is our target variable

col = [c for c in test1 if c not in ['id', 'air_store_id', 'date','visitors']]
col

#filling the null values

final1 = final1.fillna(-1)
test1 = test1.fillna(-1)

#fitting the linear regression model

lm.fit(final1[col], np.log1p(final1['visitors'].values))

#predicting the values

predict1 = lm.predict(test1[col])

predict1=pd.DataFrame(predict1)

predict1[0] = np.expm1(predict1[0]).clip(lower=0.)

predict1

predict1.to_csv('/content/drive/My Drive/ml-project/restaurant-visitor-forecasting/predict1.csv')

"""# MODEL-2 kNN"""

from sklearn import neighbors
model2= neighbors.KNeighborsRegressor(n_jobs=-1,n_neighbors=4)

#training the model
model2.fit(final1[col], np.log1p(final1['visitors'].values))

predict2 = model2.predict(test1[col])

prediction2 = pd.DataFrame(predict2)
prediction2 = np.expm1(prediction2).clip(lower=0.)

prediction2

prediction2.to_csv('/content/drive/My Drive/ml-project/restaurant-visitor-forecasting/predict2.csv')

"""# MODEL-3 Light BGM"""

#final dataset for training

final2=full
final2

#setting up the test dataset similarly

test2=sub
test2

test2["visit_date"] = pd.to_datetime(test2["visit_date"])

test2 = pd.merge(test2,date,how="left",on=["visit_date"])
test2

del test2['day_of_week']
test2

test2 = pd.merge(test2,airst,how="left",on=["air_store_id"])
test2

test2.drop({'air_area_name','visit_date'},axis=1,inplace=True)
test2

test2=pd.merge(test2,pre_std,how="left",on=["prefecture"])
test2=pd.merge(test2,pre_mean,how="left",on=["prefecture"])
test2=pd.merge(test2,gen_mean,how="left",on=["air_genre_name"])
test2=pd.merge(test2,gen_std,how="left",on=["air_genre_name"])
test2=pd.merge(test2,city_std,how="left",on=["city"])
test2=pd.merge(test2,city_mean,how="left",on=["city"])
test2=pd.merge(test2,area_mean,how='left',on=['area'])
test2=pd.merge(test2,area_std,how='left',on=['area'])
test2

test2["date"] = pd.to_datetime(test2["date"])
test2['year'] = test2['date'].dt.year
test2['month'] = test2['date'].dt.month
test2['date'] = test2['date'].dt.date
test2

test2 = test2.rename(columns={'visit_weekday':'weekday'})
test2

test2 = pd.merge(test2,stores,how="left",on=["air_store_id","weekday"])
test2

del final2['air_area_name']

y = final2['visitors']

test2 = test2.fillna(-1)
final2 = final2.fillna(-1)

categorical_feats={'air_genre_name','prefecture','area','city'}

col2 = [c for c in test2 if c not in ['id', 'air_store_id', 'date','visitors']]

import lightgbm as lgb

#tuning the hyper-parameters

hyper_params = {
    'task': 'train',
    'boosting_type': 'gbdt',
    'objective': 'regression',
    'metric': ['l2', 'auc'],
    'learning_rate': 0.05,
    'feature_fraction': 0.9,
    'bagging_fraction': 0.7,
    'bagging_freq': 10,
    'verbose': 0,
    "max_depth": 8,
    "num_leaves": 15,  
    "max_bin": 256,
    "num_iterations": 100000,
    "n_estimators": 1000
}

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(final2[col2],final2['visitors'], test_size=0.2, random_state=42)

for c in categorical_feats:
    X_train[c] = X_train[c].astype('category')
    X_test[c] = X_test[c].astype('category')
    test2[c]=test2[c].astype('category')

#training the model

lgb_train = lgb.Dataset(X_train,np.log1p(y_train))

gbm = lgb.train(hyper_params, lgb_train, num_boost_round=10, verbose_eval=False)

#predicting the values

predict3=gbm.predict(test2[col2])

prediction3

prediction3= pd.DataFrame(predict3)
prediction3[0] = np.expm1(prediction3[0])

prediction3.to_csv('/content/drive/My Drive/ml-project/restaurant-visitor-forecasting/predict3.csv')

"""## MODEL-4 XGBoost"""

from xgboost import XGBRegressor

#tuning the hyper-parameters

model3= XGBRegressor(learning_rate=0.1, random_state=3, n_estimators=500, subsample=0.8,colsample_bytree=0.8, max_depth =10,n_jobs=-1)

#training the model

model3.fit(final1[col][150000:], np.log1p(final1['visitors'][150000:].values))

#predicting the values

predict4 = model3.predict(test1[col])
prediction4 = pd.DataFrame(predict4)
prediction4 = np.expm1(prediction4).clip(lower=0.)

prediction4

prediction4.to_csv('/content/drive/My Drive/ml-project/restaurant-visitor-forecasting/predict4.csv')

"""## **ENSEMBLING**

1. kNN AND XGBOOST
"""

#we have already trained our models and made the predictions, now we will ensemble them.

ensem1 = test1[['id','visitors']].copy()
ensem1['visitors'] = 0.4*predict2+0.6*predict4
ensem1['visitors'] = np.expm1(ensem1['visitors']).clip(lower=0.)
ensem1

ensem1.to_csv('/content/drive/My Drive/ml-project/restaurant-visitor-forecasting/predict5.csv')

"""2. XGBoost, kNN and LightGBM"""

ensem2 = test1[['id','visitors']].copy()
ensem2['visitors'] = (0.1*prediction2) +(0.4*prediction3) + (0.5*prediction4)
#ensem2['visitors'] = np.expm1(ensem2['visitors']).clip(lower=0.)
ensem2

ensem2.describe()

ensem2.to_csv('/content/drive/My Drive/ml-project/restaurant-visitor-forecasting/predict6.csv')

"""3. XGBoost and LightGBM"""

ensem3 = test1[['id','visitors']].copy()
ensem3['visitors'] = 0.4*prediction3+0.6*prediction4
#ensem4['visitors'] = np.expm1(ensem4['visitors']).clip(lower=0.)
ensem3.describe()

ensem3[ensem3['visitors']>200]=200

ensem3.describe()

ensem3.to_csv('/content/drive/My Drive/ml-project/restaurant-visitor-forecasting/predict7.csv')

"""And that's it. Thankyou and happy learning!"""